{"cells":[{"metadata":{"id":"ZaFgi8LeluP2"},"cell_type":"markdown","source":"**CSI5138F - Intro to DL and RL - Project**\n\nThis notebook is heavily influenced by the following Tutorial:\n  https://mccormickml.com/2019/07/22/BERT-fine-tuning/ \n\n# Set up the notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"id":"q9r6NIXPkdk8","outputId":"5305f330-d178-4a54-c172-9fea8478a536","executionInfo":{"status":"ok","timestamp":1576071901871,"user_tz":300,"elapsed":20052,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"# install pytorch transformers\n!pip install transformers\n#!pip install pytorch_transformers\n#!pip install fairseq","execution_count":null,"outputs":[]},{"metadata":{"id":"pB4NTTHlsvk1"},"cell_type":"markdown","source":"## Imports and mount drive"},{"metadata":{"id":"TKkSu6q1lTco","outputId":"0d825f3d-d1f7-430a-81e5-ba09045fcf3b","executionInfo":{"status":"ok","timestamp":1576071926966,"user_tz":300,"elapsed":43964,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"import torch\nimport os\nimport string\nimport copy\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n#from pytorch_transformers import *\nfrom transformers import XLMRobertaForMaskedLM, XLMRobertaTokenizer\n#from fairseq import *\nimport numpy as np\nimport json\nimport collections\n#from google.colab import drive\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\n\n# Import nltk WordNet\n#import nltk\n#nltk.download('wordnet')\n#lemmatizer = nltk.stem.WordNetLemmatizer()\n\n# Mount google drive containing the datasets\n#drive.mount('/content/drive', force_remount=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from fairseq.models.roberta import XLMRModel","execution_count":null,"outputs":[]},{"metadata":{"id":"LWfqbCFIstbu"},"cell_type":"markdown","source":"## Get RoBERTa tokenizer"},{"metadata":{"id":"p_2mOPdyp7DZ","outputId":"4255f6ea-2a99-4f22-bc32-40c210ddfb6a","executionInfo":{"status":"ok","timestamp":1576071929007,"user_tz":300,"elapsed":42509,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"tokenizer =XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.encode('talking', add_special_tokens=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tok = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\ntok.convert_ids_to_tokens([459, 6496])","execution_count":null,"outputs":[]},{"metadata":{"id":"ybSNiAH_WtJA"},"cell_type":"markdown","source":"## Utility functions"},{"metadata":{"id":"HQhPLskGWrYJ","trusted":true},"cell_type":"code","source":"def load_json_objects_from_file(filename):\n  json_objects = []\n  with open(filename, mode = \"r\") as jsonl_file:\n      for line in jsonl_file:\n          json_objects.append(json.loads(line))\n  return json_objects\n\n#Takes a list of words (strings) and a sentence (as a RoBERTa tokenized ID list) and returns a list\n#of pairs indicating the tokens' start and end positions in the sentence for each word\n#NOTE: it can also apply to a group of words separated by spaces\n#NOTE: It is important that the list of words given describes a sentence, because the order is relevant to do the matching properly\n      \ndef find_word_in_tokenized_sentence(word,token_ids, wic_sentence, wic_idx):\n  #print(f\"token_ids===={token_ids}\")\n  a = tokenizer.encode(word, add_special_tokens=False)\n  decomposedWord = []\n  #print(f\"token_ids===={decomposedWord}\")\n  for x in a:\n    if x == 0 or x == 2 or x==5 or x == 6:\n        continue\n    decomposedWord.append(x)\n  if isinstance(decomposedWord, int):\n    decomposedWord = [decomposedWord]\n  #decomposedWord = [x if x != '<s>' and x != '</s>' for x in decomposedWord]\n  for i in range(len(token_ids)):\n    #print(f\"token_ids===={token_ids[i:i+len(decomposedWord)]}+decomposedWord===={decomposedWord}\")\n    #print(decomposedWord)\n    if token_ids[i] == decomposedWord[0] and token_ids[i:min(i+len(decomposedWord), len(token_ids))] == decomposedWord:\n      #print('gooodooooooooooooooooood')\n      return (i,i+len(decomposedWord)-1)\n  # This is the ouput when no matching pattern is found\n    #print(\"fuck\")\n  print(wic_idx)\n  print(f\"{tokenizer.encode(word)}_in_{token_ids}\")\n  print(f\"word====={word}+sentence===={wic_sentence}\")\n  print(f\"sentence_decoded===={tok.convert_ids_to_tokens(token_ids)}+++word_decoded{tok.convert_ids_to_tokens(decomposedWord)}\")\n  return (-1,-1)\n  \ndef find_words_in_tokenized_sentences(wordList,token_ids, wic_sentence, wic_idx):\n  intList = []\n  for word in wordList:\n    if len(intList) == 0:\n      #print(find_word_in_tokenized_sentence(word,token_ids))\n      try:\n          intList.append(find_word_in_tokenized_sentence(word,token_ids, wic_sentence, wic_idx))\n      except:\n        print(f\"{wic_idx}---nothing_found\")\n        interv = (-1, -1)\n    else:\n      afterLastInterval = intList[-1][1]+1\n      try:\n          interv = find_word_in_tokenized_sentence(word,token_ids[afterLastInterval:], wic_sentence, wic_idx)\n      except:\n        interv = (-1, -1)\n        print(f\"{wic_idx}---nothing_found\")\n      #assert(interv == (-1, -1))\n      #print(interv)\n      actualPositions = (interv[0] + afterLastInterval,interv[1]+afterLastInterval)\n      intList.append(actualPositions)\n  return intList\n\ndef last_find_words_in_tokenized_sentences(word_loc1, word_loc2, token_ids, wic_sentence, wic_idx):\n    if wic_idx == 70084 or wic_idx == 25941:\n        print(f\"{token_ids}=========={wic_sentence}\")\n    #print(wic_idx)\n    intList = []\n    sen_split = wic_sentence.split('<s>')\n    #print(wic_sentence)\n    #word_loc1[0] += 3\n    #word_loc1[1] += 3\n    word_loc1 = (word_loc1[0] + 4, word_loc1[1] + 4)\n    #word_loc2[0] += len(sen_split[1]) + 6\n    #word_loc2 += len(sen_split[1]) + 6\n    word_loc2 = (word_loc2[0] + 8 + len(sen_split[1]), word_loc2[1] + 8 + len(sen_split[1]))\n    if wic_idx == 70084 or wic_idx == 25941:\n        print(f\"{wic_sentence[word_loc1[0]:word_loc1[1]]}aaaaaandddddd{wic_sentence[word_loc2[0]:word_loc2[1]]}\")\n    decoded = tokenizer.convert_ids_to_tokens(token_ids)\n    if wic_idx == 70084 or wic_idx == 25941:\n        print(f\"{word_loc1}_and_{word_loc2}___in_________________---------{decoded}\")\n    sloc1, eloc1, sloc2, eloc2 = 0,0,0,0\n    cnt = 0\n    word1pos=[]\n    word2pos=[]\n    for i, sub in enumerate(decoded):\n        if cnt >= word_loc2[1]:\n            break\n        if cnt + len(sub) < word_loc1[0]:\n            cnt += len(sub)\n            continue\n        if cnt >= word_loc1[0] and cnt < word_loc1[1]:\n            word1pos.append(i)\n        elif cnt >= word_loc2[0] and cnt < word_loc2[1]:\n            word2pos.append(i)\n        elif cnt + len(sub) >= word_loc1[0] and cnt + len(sub) < word_loc1[1]:\n            word1pos.append(i)\n        elif cnt + len(sub) >= word_loc2[0] and cnt + len(sub) < word_loc2[1]:\n            word2pos.append(i)\n        elif cnt < word_loc1[0] and cnt + len(sub) >= word_loc1[1]:\n            word1pos.append(i)\n        elif cnt < word_loc2[0] and cnt + len(sub) >= word_loc2[1]:\n            word2pos.append(i)\n        cnt += len(sub)\n        continue\n        #if cnt + len(sub) < word_loc1[0]:\n        #    cnt += len(sub)\n        #    continue\n        #if cnt + len(sub) < word_loc2[0]:\n        #    cnt += len(sub)\n        #    continue\n        #if\n    if wic_idx == 70084 or wic_idx == 25941:\n        print(f\"{token_ids[min(word1pos):max(word1pos) + 1]}anddddddddddddddddd{token_ids[min(word2pos):max(word2pos) + 1]}\")\n    return [(min(word1pos), max(word1pos)), (min(word2pos), max(word2pos))]\n            \ndef new_find_words_in_tokenized_sentences(word_loc1, word_loc2, token_ids, wic_sentence, wic_idx):\n    intList = []\n    sen_split = wic_sentence.split('<s>')\n    len1 = len(sen_split[0])\n    part1 = tokenizer.encode(wic_sentence[:word_loc1[0] + 3].rstrip(), add_special_tokens=False)\n    part2 = tokenizer.encode(' ' + wic_sentence[word_loc1[0] + 3:word_loc1[1] + 3], add_special_tokens=False)\n    part3 = tokenizer.encode(wic_sentence[word_loc1[1] + 3: len1 + word_loc2[0] + 6].rstrip(), add_special_tokens=False)\n    part4 = tokenizer.encode(' ' + wic_sentence[len1 + word_loc2[0] + 6: len1 + word_loc2[1] + 6], add_special_tokens=False)\n    part5 = tokenizer.encode(wic_sentence[len1 + word_loc2[1] + 6:], add_special_tokens=False)\n    print(f\"Sentence converted=={tokenizer.convert_ids_to_tokens(token_ids)}+ part1===={tokenizer.convert_ids_to_tokens(part1)}\")\n    print(f\"part1=={part1}+sentence{token_ids}\")\n    spos1 = 0\n    epos1 = 0\n    spos2, epos2 = 0, 0\n    while len(part1) > 0 and token_ids[0] == part1[0]:\n        token_ids.pop(0)\n        part1.pop(0)\n        spos1 += 1\n        spos2 += 1\n    if len(part1) > 0:\n        print(\"Not part1\")\n        print(wic_idx)\n        print(wic_sentence)\n    epos1 = spos1\n    print(part3)\n    print(token_ids)\n    while part3[0] != token_ids[0]:\n        token_ids.pop(0)\n        epos1 += 1\n    spos2 = epos1\n    while len(part3) > 0 and token_ids[0] == part3[0]:\n        token_ids.pop(0)\n        part3.pop(0)\n        spos2 += 1\n    epos2 = spos2\n    print(f\"part5=={part5}+decoded{tokenizer.convert_ids_to_tokens(part5)}\")\n    while part5[0] != token_ids[0]:\n        token_ids.pop(0)\n        epos2 += 1\n    return [(spos1, epos1), (spos2, epos2)]\n    \n    \n    \n\ndef flat_accuracy(preds, labels, return_predict_correctness = False):\n  pred_flat = np.argmax(preds, axis=1).flatten()\n  labels_flat = labels.flatten()\n  if return_predict_correctness:\n    return np.sum(pred_flat == labels_flat) / len(labels_flat), pred_flat == labels_flat\n  else:\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ndef flat_predictions(preds):\n  pred_flat = np.argmax(preds, axis=1).flatten()\n  return pred_flat == 1","execution_count":null,"outputs":[]},{"metadata":{"id":"XT_dMJ2vr7rd"},"cell_type":"markdown","source":"## Constants"},{"metadata":{"id":"xwNPw0J6r6c1","outputId":"2945624d-dda1-49b1-e43a-8a939e1a89ed","executionInfo":{"status":"ok","timestamp":1576071954080,"user_tz":300,"elapsed":968,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nEPOCHS = 80\nPATIENCE = 6\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATIENCE = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"id":"sgPaP-U9zOGO"},"cell_type":"markdown","source":"## Import and Process the WordInContext Training data"},{"metadata":{"id":"UFrrDdukUvwn","trusted":true},"cell_type":"code","source":"# Create a function to preprocess the WiC data\nbad_arr = []\nidx_arr = []\nbad_objects = []\ndef wic_preprocessing(json_objects, training = True, shuffle_data = False, verbose = False):\n  wic_sentences = []\n  wic_encoded = []\n  wic_labels = []\n  wic_word_locs = []\n  wic_indexes = []\n  for index, example in tqdm(enumerate(json_objects)):\n    if example['idx'] in [8761, 14399, 3460, 14070, 35431, 34543]:\n        continue\n    #print(index)\n    #if index == 86176:\n    #    print(example)\n    if example['idx'] == 70084:\n        continue\n    wic_indexes.append(example['idx']) \n    #wic_indexes.append(index)\n    #print(example['sentence1'])\n    sentence = f\"<s>{example['sentence1']}</s><s>{example['sentence2']}</s>\"\n    if len(sentence) > 1000:\n        bad_objects.append(example)\n        bad_arr.append(len(sentence))\n        idx_arr.append(example['idx'])\n    #if index == 14934:\n    #    print(sentence)\n    wic_sentences.append(sentence)\n    # encoded the sentences\n    wic_encoded.append(tokenizer.encode(sentence, add_special_tokens=False))\n    word = example['word']\n    word_locs = (-1, -1)\n    sent1_split = example['sentence1'].split(' ')\n    sent2_split = example['sentence2'].split(' ')\n    #sent1_split = ['_' + x for x in sent1_split]\n    #sent2_split = ['_' + x for x in sent2_split]\n    \n    # Get the index of word in both sentences\n    sent1_word_char_loc = (example['start1'], example['end1'])\n    sent2_word_char_loc = (example['start2'], example['end2'])\n    # Create a variable to keep track of the number of characters parsed in each sentence as we loop\n    sent_chars = 0\n    i, j = 0, 0\n    word1_not_found, word2_not_found = True, True\n    while word1_not_found and i < len(sent1_split):\n      word_len = len(sent1_split[i])\n      if sent_chars >= sent1_word_char_loc[0] or sent_chars + word_len >= sent1_word_char_loc[1]:\n        word_locs = (i, -1) # Found the word in the sentence\n        word1_not_found = False\n      elif sent_chars > sent1_word_char_loc[1]:\n        # If we got past the word. Assume it was the previous word\n        word_locs = (i - 1, -1) # Found the word in the sentence\n        word1_not_found = False\n      else:\n        # Look at the next word\n        sent_chars += word_len + 1 # Plus one for the space\n        i += 1\n    # Loop over the words in the second\n    sent_chars = 0 # Reset\n    while word2_not_found and j < len(sent2_split):\n      word_len = len(sent2_split[j])\n      if sent_chars >= sent2_word_char_loc[0] or sent_chars + word_len >= sent2_word_char_loc[1]:\n        word_locs = (i, j) # Found the word in the sentence\n        word2_not_found = False\n      elif sent_chars > sent2_word_char_loc[1]:\n        # If we somehow got past the word. Assume it was the previous word\n        word_locs = (i, j - 1) # Found the word in the sentence\n        word2_not_found = False\n      else:\n        # Look at the next word\n        sent_chars += word_len + 1 # Plus one for the space\n        j += 1\n    # For testing\n    if verbose:\n      print(word)\n      print(sent1_split)\n      print(sent2_split)\n      print(word_locs)\n    # Now to find the word in the tokenized sentences\n    word1 = example['sentence1'][example['start1']:example['end1']]#[#sent1_split[word_locs[0]]#.translate(str.maketrans('', '', string.punctuation)) #Remove punctuation\n    word2 = example['sentence2'][example['start2']:example['end2']]#sent2_split[word_locs[1]]#.translate(str.maketrans('', '', string.punctuation)) #Remove punctuation\n    #print(word1, word2)\n    token_word_locs = last_find_words_in_tokenized_sentences(sent1_word_char_loc, sent2_word_char_loc, wic_encoded[-1], wic_sentences[-1], wic_indexes[-1])\n    wic_word_locs.append(token_word_locs)\n    # Get the label if we expect it to be there\n    if training:\n      if example['label']:\n        wic_labels.append(1)\n      else:\n        wic_labels.append(0)\n  # Pad the sequences and find the encoded word location in the combined input\n  max_len = np.array([len(ex) for ex in wic_encoded]).max()\n  wic_padded = {\"input_ids\" : [], \"attention_mask\" : [], \"token_type_ids\" : [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : wic_indexes}\n  for i in tqdm(range(0, len(wic_encoded))):\n    enc_sentence = wic_encoded[i]\n    word_locs = wic_word_locs[i]\n    # Pad the sequences\n    ex_len = len(enc_sentence)\n    padded_sentence = enc_sentence\n    padded_sentence.extend([0]*(max_len - ex_len))\n    wic_padded[\"input_ids\"].append(padded_sentence)\n    padded_mask = [1] * ex_len\n    padded_mask.extend([0]*(max_len - ex_len))\n    wic_padded[\"attention_mask\"].append(padded_mask)\n    token_word_locs = wic_word_locs[i]\n    first_word_loc = []\n    second_word_loc = []\n    len_first_word = token_word_locs[0][1] - token_word_locs[0][0] + 1\n    len_second_word = token_word_locs[1][1] - token_word_locs[1][0] + 1\n    for j in range(0, max_len):\n      if j >= token_word_locs[0][0] and j <= token_word_locs[0][1]:\n        #Part of the first word\n        first_word_loc.append(1.0)# / len_first_word)\n      else:\n        first_word_loc.append(0.0)\n      if j >= token_word_locs[1][0] and j <= token_word_locs[1][1]:\n        #Part of the second word\n        second_word_loc.append(1.0)# / len_second_word)\n      else:\n        second_word_loc.append(0.0)\n    # We want to append a [1, max_len] vector instead of a [max_len] vector so wrap in an array\n    wic_padded[\"word1_locs\"].append([first_word_loc])\n    wic_padded[\"word2_locs\"].append([second_word_loc])\n    # token_type_ids is a mask that tells where the first and second sentences are\n    token_type_id = []\n    first_sentence = True\n    sentence_start = True\n    for token in padded_sentence:\n      if first_sentence and sentence_start and token == 0:\n        # Allows 0 at the start of the first sentence\n        token_type_id.append(0)\n      elif first_sentence and token > 0:\n        if sentence_start:\n          sentence_start = False\n        token_type_id.append(0)\n      elif first_sentence and not sentence_start and token == 0:\n        first_sentence = False\n        # Start of second sentence\n        token_type_id.append(1)\n      else:\n        # Second sentence\n        token_type_id.append(1)\n    wic_padded[\"token_type_ids\"].append(token_type_id)\n  if training:\n    if shuffle_data:\n      # Shuffle the data\n      raw_set = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": [], \"labels\": [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : []}\n      raw_set[\"input_ids\"], raw_set[\"token_type_ids\"], raw_set[\"attention_mask\"], raw_set[\"labels\"], raw_set[\"word1_locs\"], raw_set[\"word2_locs\"], raw_set[\"index\"] = shuffle(\n          wic_padded[\"input_ids\"], wic_padded[\"token_type_ids\"], wic_padded[\"attention_mask\"], wic_labels, wic_padded[\"word1_locs\"], wic_padded[\"word2_locs\"], wic_padded[\"index\"])\n    else:\n      raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"],\n                 \"attention_mask\": wic_padded[\"attention_mask\"], \"labels\": wic_labels, \"index\" : wic_padded[\"index\"],\n                 \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n  else: # No labels present (Testing set)\n    # Do not shuffle the testing set\n    raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"], \n               \"attention_mask\": wic_padded[\"attention_mask\"], \"index\" : wic_padded[\"index\"], \n               \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n  # Return the raw data (Need to put them in a PyTorch tensor and dataset)\n  return raw_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(train_json_objs, test_size=5000, random_state=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test)","execution_count":null,"outputs":[]},{"metadata":{"id":"pcN2TSeax3C4","outputId":"686ff500-aa93-4cc0-dd8f-15aa3647f94f","executionInfo":{"status":"ok","timestamp":1576071933637,"user_tz":300,"elapsed":39056,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"# Process the data\ntrain_json_objs = load_json_objects_from_file(\"../input/5-mcl-wic/mcl_wic_new/Russe_Wic_MCL_wic_XL-Ch_Xl-fr.jsonl\")\ntrain, test = train_test_split(train_json_objs, test_size=23000, random_state=8)\nraw_train_set = wic_preprocessing(train, shuffle_data=False, verbose = False) # We do not want to shuffle for now.\nprint(train_json_objs[raw_train_set[\"index\"][15]])\nprint(raw_train_set[\"input_ids\"][15]),\nprint(raw_train_set[\"token_type_ids\"][15]),\nprint(raw_train_set[\"attention_mask\"][15]),\nprint(raw_train_set[\"labels\"][15])\nprint(raw_train_set[\"word1_locs\"][15]) #right, so it's better to use one-hot vectors to do a selection afterwards, I can compute it more precisely with the token intervals however\nprint(raw_train_set[\"word2_locs\"][15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_objects","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process the data\ntrain_json_objs = load_json_objects_from_file(\"../input/wic-second/WiC/train.jsonl\")\nraw_train_set = wic_preprocessing(train_json_objs, shuffle_data=True, verbose = False) # We do not want to shuffle for now.\nprint(train_json_objs[raw_train_set[\"index\"][15]])\nprint(raw_train_set[\"input_ids\"][15]),\nprint(raw_train_set[\"token_type_ids\"][15]),\nprint(raw_train_set[\"attention_mask\"][15]),\nprint(raw_train_set[\"labels\"][15])\nprint(raw_train_set[\"word1_locs\"][15]) #right, so it's better to use one-hot vectors to do a selection afterwards, I can compute it more precisely with the token intervals however\nprint(raw_train_set[\"word2_locs\"][15])","execution_count":null,"outputs":[]},{"metadata":{"id":"edSHPIYgM_yv","outputId":"08925c70-fbf0-47d5-91b1-75e3c2ade0c8","executionInfo":{"status":"ok","timestamp":1576071968588,"user_tz":300,"elapsed":803,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"print(len(raw_train_set[\"labels\"])/BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"Ag_HFfjYpRTc"},"cell_type":"markdown","source":"### Load into a PyTorch dataset"},{"metadata":{"id":"jNmRgSPlpRol","trusted":true},"cell_type":"code","source":"\ntrain_data = TensorDataset(\n    torch.tensor(raw_train_set[\"input_ids\"]),\n    torch.tensor(raw_train_set[\"token_type_ids\"]),\n    torch.tensor(raw_train_set[\"attention_mask\"]),\n    torch.tensor(raw_train_set[\"labels\"]),\n    torch.tensor(raw_train_set[\"word1_locs\"]),\n    torch.tensor(raw_train_set[\"word2_locs\"]),\n    torch.tensor(raw_train_set[\"index\"])\n)\n\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"id":"7ds89NLKOuP6"},"cell_type":"markdown","source":"### Repeat the procedure to load the WiC validation and testing sets"},{"metadata":{"id":"FoLbCX9uO9BW","outputId":"c83a36f2-4d21-4295-ebfa-b810807b31ce","executionInfo":{"status":"ok","timestamp":1576071998651,"user_tz":300,"elapsed":3161,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"# Load the json objects from each file\n#test_json_objs = load_json_objects_from_file(\"../input/mcl-wic-1/mcl_wic_json/test.jsonl\")[0]\n#print(test_json_objs)\n#valid_json_objs = load_json_objects_from_file(\"../input/mcl-wic-1/mcl_wic_json/val.jsonl\")[0]\n# Process the objects\nprint(len(test))\n#raw_test_set = wic_preprocessing(test_json_objs, training = False, verbose=False) # The labels for the testing set are unknown\nraw_valid_set = wic_preprocessing(test)\n# Create PyTorch datasets\n#test_data = TensorDataset(\n#    torch.tensor(raw_test_set[\"input_ids\"]),\n#    torch.tensor(raw_test_set[\"token_type_ids\"]),\n#    torch.tensor(raw_test_set[\"attention_mask\"]),\n#    torch.tensor(raw_test_set[\"word1_locs\"]),\n#    torch.tensor(raw_test_set[\"word2_locs\"]),\n#    torch.tensor(raw_test_set[\"index\"])\n#)\nvalidation_data = TensorDataset(\n    torch.tensor(raw_valid_set[\"input_ids\"]),\n    torch.tensor(raw_valid_set[\"token_type_ids\"]),\n    torch.tensor(raw_valid_set[\"attention_mask\"]),\n    torch.tensor(raw_valid_set[\"labels\"]),\n    torch.tensor(raw_valid_set[\"word1_locs\"]),\n    torch.tensor(raw_valid_set[\"word2_locs\"]),\n    torch.tensor(raw_valid_set[\"index\"])\n)\n# Create a sampler and loader for each\n#test_sampler = RandomSampler(test_data)\n#test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=4)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_arr","execution_count":null,"outputs":[]},{"metadata":{"id":"81Bpf7MsNeJI","outputId":"65074be0-d674-4e61-9022-6fe9dafb63fc","executionInfo":{"status":"ok","timestamp":1576071998652,"user_tz":300,"elapsed":1446,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"print(len(raw_valid_set[\"labels\"])/4)","execution_count":null,"outputs":[]},{"metadata":{"id":"tojdzdhOpZh-"},"cell_type":"markdown","source":"# RoBERTa"},{"metadata":{"id":"lwL5Ma6WaxWD"},"cell_type":"markdown","source":"## Loading the model"},{"metadata":{"id":"RxGfvKkdvAvs","outputId":"a06e0282-ce56-4f74-930b-622214cb4389","executionInfo":{"status":"ok","timestamp":1576006101588,"user_tz":300,"elapsed":18966,"user":{"displayName":"William LaRocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBQdRLVXLOQdo2d4615BWNsTFeI-WG1EkEFuhISsA=s64","userId":"15267085529970099831"}},"trusted":true},"cell_type":"code","source":"\nmodel = XLMRobertaForMaskedLM.from_pretrained('xlm-roberta-base')\n#roberta_init_weights = model.state_dict()","execution_count":null,"outputs":[]},{"metadata":{"id":"Tpz_9Vj7HxZx"},"cell_type":"markdown","source":"## Create a custom head for WiC instead of a classification head\nBased on https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html and https://huggingface.co/transformers/_modules/transformers/modeling_roberta.html#RobertaModel\n\n"},{"metadata":{"id":"SNE06UspH9Sk","trusted":true},"cell_type":"code","source":"class WiC_Head(torch.nn.Module):\n    def __init__(self, roberta_based_model, embedding_size = 1024):\n        \"\"\"\n        Keeps a reference to the provided RoBERTa model. \n        It then adds a linear layer that takes the distance between two \n        \"\"\"\n        super(WiC_Head, self).__init__()\n        self.embedding_size = embedding_size\n        self.embedder = roberta_based_model\n        self.linear_diff = torch.nn.Linear(embedding_size, 250, bias = True)\n        self.linear_seperator = torch.nn.Linear(250, 2, bias = True)\n        self.loss = torch.nn.CrossEntropyLoss()\n        self.activation = torch.nn.ReLU()\n        self.softmax = torch.nn.Softmax()\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None,\n                word1_locs = None, word2_locs = None):\n        \"\"\"\n        Takes in the same argument as RoBERTa forward plus two tensors for the location of the 2 words to compare\n        \"\"\"\n        if word1_locs is None or word2_locs is None:\n          raise ValueError(\"The tensors (word1_locs, word1_locs) containing the location of the words to compare in the input vector must be provided.\")\n        elif input_ids is None:\n          raise ValueError(\"The input_ids tensor must be provided.\")\n        elif word1_locs.shape[0] != input_ids.shape[0] or word2_locs.shape[0] != input_ids.shape[0]:\n          raise ValueError(\"All provided vectors should have the same batch size.\")\n        batch_size = word1_locs.shape[0]\n        embs, _ = self.embedder.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        word1s = torch.matmul(word1_locs, embs).view(batch_size, self.embedding_size)\n        word2s = torch.matmul(word2_locs, embs).view(batch_size, self.embedding_size)\n        diff = word1s - word2s\n        layer1_results = self.activation(self.linear_diff(diff))\n        logits = self.softmax(self.linear_seperator(layer1_results))\n        outputs = logits\n        # Losss\n        if labels is not None:\n            loss = self.loss(logits.view(-1, 2), labels.view(-1))\n            outputs = (loss, logits)\n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"id":"rV3pQevlv1Ga"},"cell_type":"markdown","source":"## Get model"},{"metadata":{"id":"iD7tdj3d50J1","trusted":true},"cell_type":"code","source":"class_model = WiC_Head(model, embedding_size = 768)","execution_count":null,"outputs":[]},{"metadata":{"id":"myJ08VFvzBXg"},"cell_type":"markdown","source":"## The training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoModelWithLMHead, AutoTokenizer\n\nmodel = AutoModelWithLMHead.from_pretrained(\"t5-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n\ninputs = tokenizer.encode(\"translate English to German: Hugging Face is a technology company based in New York and Paris\", return_tensors=\"pt\")\noutputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n\nprint(outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.decode(outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_model.embedder.requires_grad_ = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(class_model.embedder.requires_grad_)","execution_count":null,"outputs":[]},{"metadata":{"id":"6jHBnivtzCOf","outputId":"a40dae1c-bb22-43cc-eeac-4b664cf3c420","executionInfo":{"status":"ok","timestamp":1576008705184,"user_tz":300,"elapsed":1226337,"user":{"displayName":"William LaRocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBQdRLVXLOQdo2d4615BWNsTFeI-WG1EkEFuhISsA=s64","userId":"15267085529970099831"}},"trusted":true},"cell_type":"code","source":"\nMIN_ACCURACY = 0.99 \nREACHED_MIN_ACCURACY = False\nbest_weights = class_model.state_dict()\nmax_val_acc = (0, 0)\nclass_model.cuda()\n\nparam_optimizer = list(class_model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\nfit_history = {\"loss\": [],  \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\nepoch_number = 0\nepoch_since_max = 0\ncontinue_learning = True\nwhile epoch_number < EPOCHS and continue_learning:\n  epoch_number += 1\n  print(f\"Training epoch #{epoch_number}\")\n  tr_loss, tr_accuracy = 0, 0\n  nb_tr_examples, nb_tr_steps = 0, 0\n  eval_loss, eval_accuracy = 0, 0\n  nb_eval_steps, nb_eval_examples = 0, 0\n  # Training\n  \n  class_model.train()\n  # Freeze weights\n  class_model.embedder.requires_grad_ = False\n  #class_model.embedder.requires_grad_ = True\n  for step, batch in enumerate(train_dataloader):\n    batch = tuple(t.cuda() for t in batch)\n    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n    optimizer.zero_grad() \n    loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n                               labels=b_labels, word1_locs = b_word1, word2_locs = b_word2) \n    loss.backward()\n\n    optimizer.step()\n\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.cpu().numpy()\n    # Calculate the accuracy\n    b_accuracy = flat_accuracy(logits, label_ids) # For RobertaForClassification\n    # Append to fit history\n    fit_history[\"loss\"].append(loss.item()) \n    fit_history[\"accuracy\"].append(b_accuracy) \n    # Update tracking variables\n    tr_loss += loss.item()\n    tr_accuracy += b_accuracy\n    nb_tr_examples += b_input_ids.size(0)\n    nb_tr_steps += 1\n    if nb_tr_steps%10 == 0:\n      print(\"\\t\\tTraining Batch {}: Loss: {}; Accuracy: {}\".format(nb_tr_steps, loss.item(), b_accuracy))\n  print(\"Training:\\n\\tLoss: {}; Accuracy: {}\".format(tr_loss/nb_tr_steps, tr_accuracy/nb_tr_steps))\n  # Validation\n  class_model.eval()\n  for batch in validation_dataloader:\n    batch = tuple(t.cuda() for t in batch)\n    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n    with torch.no_grad():\n      #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)\n      loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n                                 labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.cpu().numpy()\n  \n    b_accuracy = flat_accuracy(logits, label_ids)\n    \n    fit_history[\"val_loss\"].append(loss.item()) \n    fit_history[\"val_accuracy\"].append(b_accuracy) \n    # Update tracking variables\n    eval_loss += loss.item()\n    eval_accuracy += b_accuracy\n    nb_eval_examples += b_input_ids.size(0)\n    nb_eval_steps += 1\n    if nb_eval_steps%10 == 0:\n      print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n  eval_acc = eval_accuracy/nb_eval_steps\n  if eval_acc >= max_val_acc[0]:\n    max_val_acc = (eval_acc, epoch_number)\n    continue_learning = True\n    epoch_since_max = 0 \n    best_weights = copy.deepcopy(class_model.state_dict()) \n    if eval_acc >= MIN_ACCURACY:\n      REACHED_MIN_ACCURACY = True\n    if REACHED_MIN_ACCURACY:\n      torch.save(best_weights, os.path.join(PATH,'WiCHead.pt'))\n      continue_learning = False \n  else:\n    epoch_since_max += 1\n    if epoch_since_max > PATIENCE:\n      continue_learning = False \n  print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\nprint(f\"Best accuracy ({max_val_acc[0]}) obtained at epoch #{max_val_acc[1]}.\")\n\nclass_model.load_state_dict(best_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"load_json_objects_from_file('../input/semeval/MCL-WiC/dev/multilingual/dev.ar-ar.data')","execution_count":null,"outputs":[]},{"metadata":{"id":"ZM2mislwzxU2"},"cell_type":"markdown","source":"### Save fit history to json file"},{"metadata":{"id":"JpDdoulizweS","trusted":true},"cell_type":"code","source":"with open(os.path.join(PATH, \"fit_history.json\"), 'w') as json_file:\n  json.dump(fit_history, json_file)","execution_count":null,"outputs":[]},{"metadata":{"id":"ADe38EzsUEdW"},"cell_type":"markdown","source":"### Save a model that was close to the baseline (Manual choice)"},{"metadata":{"id":"NQ4s80JbUMl9","trusted":true},"cell_type":"code","source":"# Save the best weights to file\n# torch.save(best_weights, os.path.join(PATH,'WiCHead.pt'))","execution_count":null,"outputs":[]},{"metadata":{"id":"r_fawcM75-3F"},"cell_type":"markdown","source":"## Load an already trained WiC model from file"},{"metadata":{"id":"ueIc_Z2a6EYH","trusted":true},"cell_type":"code","source":"# Load the model\n#class_model.load_state_dict(torch.load(os.path.join(PATH,'WiCHead.pt')))\n#class_model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"id":"Y8IpbYDopqrP"},"cell_type":"markdown","source":"## Get the predictions on the validation set"},{"metadata":{"id":"Z5zIry_HprRH","outputId":"c0b4f40a-fb97-46fe-e01b-462e67702546","executionInfo":{"status":"ok","timestamp":1575748120419,"user_tz":300,"elapsed":5468,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"validation_predictions_correctness = {}\n# Validation\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n# Put model in evaluation mode\nclass_model.eval()\n# Evaluate data for one epoch\nfor batch in validation_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.cuda() for t in batch)\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n  with torch.no_grad():\n    # Forward pass, calculate logit predictions\n    #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)\n    loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n                                labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  label_ids = b_labels.cpu().numpy()\n  # Calculate the accuracy\n  b_accuracy, b_pred_correctness = flat_accuracy(logits, label_ids, return_predict_correctness = True) # For RobertaForClassification\n  indexes = b_index.detach().cpu().numpy() # Get the indexes\n  # Add to predictions\n  for index, pred in zip(indexes, b_pred_correctness):\n    validation_predictions_correctness[index] = pred\n  # Update tracking variables\n  eval_loss += loss.item()\n  eval_accuracy += b_accuracy\n  nb_eval_examples += b_input_ids.size(0)\n  nb_eval_steps += 1\n  if nb_eval_steps%10 == 0:\n    print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\nprint(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\nvalidation_predictions_correctness = collections.OrderedDict(sorted(validation_predictions_correctness.items()))\nprint(validation_predictions_correctness)","execution_count":null,"outputs":[]},{"metadata":{"id":"gZwHpGrpYhj5"},"cell_type":"markdown","source":"## Get the testing results\n**Works for now. Would need to be modified to submit for competition reward.**\n\nSee https://competitions.codalab.org/competitions/20010#learn_the_details-evaluation\n\n**Update:** The jsonl file from superglue is not in the same order as the text file taken from WiC. It doesn't match the idx parameter either. That is weird."},{"metadata":{"id":"OLCnprHcYgxj","outputId":"63ac9d6f-38d4-4399-9a55-5ab643333ffc","executionInfo":{"status":"ok","timestamp":1575744955259,"user_tz":300,"elapsed":10048,"user":{"displayName":"William Larocque","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6-iq58yGKV4kpzcKwx_L1gtUj3qGLWBvAq5Bw=s64","userId":"17837505087785852856"}},"trusted":true},"cell_type":"code","source":"test_predictions = {}\nnb_test_examples, nb_test_steps = 0, 0\n# Testing\n# Put model in evaluation mode to evaluate loss on the validation set\nclass_model.eval()\n# Evaluate data for one epoch\nfor batch in test_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.cuda() for t in batch)\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_token_ids, b_input_mask, b_word1, b_word2, b_index = batch\n  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n  with torch.no_grad():\n    # Forward pass, calculate logit predictions\n    logits = class_model(b_input_ids, attention_mask=b_input_mask, word1_locs = b_word1, word2_locs = b_word2)\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  # Get the predictions\n  b_preds = flat_predictions(logits)\n  indexes = b_index.detach().cpu().numpy() # Get the indexes\n  for index, pred in zip(indexes, b_preds):\n    test_predictions[index] = pred\n  # Update tracking variables\n  nb_test_examples += b_input_ids.size(0)\n  nb_test_steps += 1\n  if nb_test_steps%10 == 0:\n    print(\"\\t\\tTest Batch {}\".format(nb_test_steps))\n# Print final results\nprint(\"Testing Done!\")\ntest_predictions = collections.OrderedDict(sorted(test_predictions.items()))\nprint(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}